---
title: Coding - It's (Not) the Same Old Song
layout: post
description: Keyword - Perception
categories: none
---
First off, apologies. I have a terrible habit of writing the blog post without actually posting it, hence the lateness of this entry.

Like [Joe](http://joetorok.github.io/blog/2016-02-17/sedimenting-knowledge.html), I’ll try to relate the coding back to the Manovich, since he’s the first of our theorist to actually address coding itself. Joe brings up some interesting questions of “modularity” in his post, although I’m slightly confused by what he means when he says, **“I have a stronger sense of sedimenting modularity than I do with binary computing.”** Isn't the binary also somewhat modular? All those 0's and 1's also have to do with "the fractal structure of new media" (30) right? Or perhaps I'm confused because I still am unsure of what I consider to be the computer’s “independent parts.” I know, for instance, that a variable and a function are different, and I can articulate their differences (in words), but when it comes down to it, they still look awfully alike to me. 

There are some intriguing questions about the way the human brain works here. Manovich addresses this predominantly in his section on “numerical representation,” especially his distinction between the analog and the digital. I’m unclear here if data is naturally perceived “continuously” (like in photographs) or “discretely” (as in the digital). Or does our involvement with new media cause us to perceive things *more* discretely (as an example of “transcoding” that he discusses later)? Are some humans more inclined to perceive data one way or the other? I guess what I’m asking is, why are there some humans who are naturally more adept at coding than others?

As someone who is working in the study of literature, I feel like my brain has been trained to perceive data continuously. I’m inclined to “narrativize” (which I discussed in my earlier post). I have a hard time getting at the “discrete units” of p5.js; I freak out when I mistakenly replace one small part (let’s say, a comma) with another small part (a semicolon) and the whole system breaks down. Can’t the computer assume, from the rest of my coding “narrative,” what I really meant to put there? If I say “snowfall” in the beginning, isn’t the computer smart enough to figure out that I really want snow instead of bubbles, even if I don’t go in and replace every single one of the subsequent parts? 

I don’t *really* want the computer to be smarter than me, that is, until I *need* it to be smarter than me. And the only reason I really *need* the computer to be smarter than me is so I can submit my dailies and move on to things that are more important to me. Like books. 

This then begs the question: if I’m trying to “narrate” coding, am I also “digitizing,” to some extent, what I subsequently read? Is a class like this reinforcing transcoding (that is, for Manovich, already happening everywhere anyway)? Or is it just making me more aware of that it's happening? 
